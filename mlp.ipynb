{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Train Loss: 0.290282 | Val Loss: 0.042751\n",
      "Epoch [2/50] | Train Loss: 0.119889 | Val Loss: 0.055088\n",
      "Epoch [3/50] | Train Loss: 0.061540 | Val Loss: 0.047790\n",
      "Epoch [4/50] | Train Loss: 0.056029 | Val Loss: 0.044460\n",
      "Epoch [5/50] | Train Loss: 0.046353 | Val Loss: 0.033554\n",
      "Epoch [6/50] | Train Loss: 0.044093 | Val Loss: 0.032425\n",
      "Epoch [7/50] | Train Loss: 0.044583 | Val Loss: 0.032435\n",
      "Epoch [8/50] | Train Loss: 0.043270 | Val Loss: 0.033736\n",
      "Epoch [9/50] | Train Loss: 0.041887 | Val Loss: 0.032433\n",
      "Epoch [10/50] | Train Loss: 0.041890 | Val Loss: 0.032062\n",
      "Epoch [11/50] | Train Loss: 0.040818 | Val Loss: 0.032618\n",
      "Epoch [12/50] | Train Loss: 0.041974 | Val Loss: 0.032241\n",
      "Epoch [13/50] | Train Loss: 0.041986 | Val Loss: 0.032529\n",
      "Epoch [14/50] | Train Loss: 0.041525 | Val Loss: 0.032035\n",
      "Epoch [15/50] | Train Loss: 0.042033 | Val Loss: 0.031813\n",
      "Epoch [16/50] | Train Loss: 0.040470 | Val Loss: 0.031939\n",
      "Epoch [17/50] | Train Loss: 0.041477 | Val Loss: 0.032378\n",
      "Epoch [18/50] | Train Loss: 0.040022 | Val Loss: 0.032467\n",
      "Epoch [19/50] | Train Loss: 0.041209 | Val Loss: 0.032063\n",
      "Epoch [20/50] | Train Loss: 0.040990 | Val Loss: 0.032123\n",
      "Epoch [21/50] | Train Loss: 0.041199 | Val Loss: 0.032135\n",
      "Epoch [22/50] | Train Loss: 0.041430 | Val Loss: 0.033443\n",
      "Epoch [23/50] | Train Loss: 0.040539 | Val Loss: 0.032658\n",
      "Epoch [24/50] | Train Loss: 0.040308 | Val Loss: 0.032523\n",
      "Epoch [25/50] | Train Loss: 0.040458 | Val Loss: 0.032906\n",
      "Epoch [26/50] | Train Loss: 0.041669 | Val Loss: 0.032977\n",
      "Epoch [27/50] | Train Loss: 0.040995 | Val Loss: 0.032811\n",
      "Epoch [28/50] | Train Loss: 0.040408 | Val Loss: 0.033001\n",
      "Epoch [29/50] | Train Loss: 0.040536 | Val Loss: 0.033188\n",
      "Epoch [30/50] | Train Loss: 0.039552 | Val Loss: 0.032918\n",
      "Epoch [31/50] | Train Loss: 0.041440 | Val Loss: 0.032980\n",
      "Epoch [32/50] | Train Loss: 0.038675 | Val Loss: 0.032883\n",
      "Epoch [33/50] | Train Loss: 0.040675 | Val Loss: 0.033083\n",
      "Epoch [34/50] | Train Loss: 0.039932 | Val Loss: 0.032872\n",
      "Epoch [35/50] | Train Loss: 0.039500 | Val Loss: 0.032878\n",
      "Epoch [36/50] | Train Loss: 0.040328 | Val Loss: 0.033116\n",
      "Epoch [37/50] | Train Loss: 0.040404 | Val Loss: 0.033164\n",
      "Epoch [38/50] | Train Loss: 0.039755 | Val Loss: 0.033096\n",
      "Epoch [39/50] | Train Loss: 0.040838 | Val Loss: 0.032873\n",
      "Epoch [40/50] | Train Loss: 0.041155 | Val Loss: 0.033127\n",
      "Epoch [41/50] | Train Loss: 0.040412 | Val Loss: 0.032609\n",
      "Epoch [42/50] | Train Loss: 0.040665 | Val Loss: 0.032917\n",
      "Epoch [43/50] | Train Loss: 0.040643 | Val Loss: 0.033470\n",
      "Epoch [44/50] | Train Loss: 0.040691 | Val Loss: 0.032801\n",
      "Epoch [45/50] | Train Loss: 0.040184 | Val Loss: 0.032603\n",
      "Epoch [46/50] | Train Loss: 0.040107 | Val Loss: 0.032800\n",
      "Epoch [47/50] | Train Loss: 0.039830 | Val Loss: 0.033095\n",
      "Epoch [48/50] | Train Loss: 0.039853 | Val Loss: 0.032919\n",
      "Epoch [49/50] | Train Loss: 0.038880 | Val Loss: 0.032729\n",
      "Epoch [50/50] | Train Loss: 0.040203 | Val Loss: 0.033202\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, List\n",
    "\n",
    "def batch_generator(train_x: np.ndarray, train_y: np.ndarray, batch_size: int):\n",
    "    \"\"\"\n",
    "    Generator that yields mini-batches of data from the training set.\n",
    "\n",
    "    This helps in iterating over the training data in batches, which is useful for\n",
    "    mini-batch stochastic gradient descent.\n",
    "\n",
    "    :param train_x: Input features array of shape (n_samples, n_features).\n",
    "    :param train_y: Target values array of shape (n_samples, n_outputs).\n",
    "    :param batch_size: Number of samples per batch.\n",
    "    :yield: A tuple (batch_x, batch_y) for each batch.\n",
    "    \"\"\"\n",
    "    n = train_x.shape[0]\n",
    "    for start_idx in range(0, n, batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_x = train_x[start_idx:end_idx]\n",
    "        batch_y = train_y[start_idx:end_idx]\n",
    "        yield batch_x, batch_y\n",
    "\n",
    "\n",
    "##############################################################################################################################\n",
    "#                                               Activation Functions (Abstract Base Class)                                 #\n",
    "##############################################################################################################################\n",
    "\n",
    "class ActivationFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the activation function for the given input.\n",
    "        \n",
    "        :param x: Input array.\n",
    "        :return: Activated output.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function with respect to the input.\n",
    "        \n",
    "        :param x: Input array.\n",
    "        :return: Derivative of the activation function evaluated at x.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Sigmoid(ActivationFunction):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the sigmoid activation: 1 / (1 + exp(-x)).\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the sigmoid function.\n",
    "        \n",
    "        The derivative is: sigmoid(x) * (1 - sigmoid(x)).\n",
    "        \"\"\"\n",
    "        sig = self.forward(x)\n",
    "        return sig * (1.0 - sig)\n",
    "\n",
    "\n",
    "class Tanh(ActivationFunction):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the hyperbolic tangent activation.\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the tanh function.\n",
    "        \n",
    "        The derivative is: 1 - tanh(x)^2.\n",
    "        \"\"\"\n",
    "        return 1.0 - np.tanh(x)**2\n",
    "\n",
    "\n",
    "class Relu(ActivationFunction):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the ReLU activation: max(0, x).\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the ReLU function.\n",
    "        \n",
    "        The derivative is 1 for x > 0 and 0 otherwise.\n",
    "        \"\"\"\n",
    "        return (x > 0).astype(x.dtype)\n",
    "\n",
    "\n",
    "class Softmax(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Softmax activation is generally used in the final layer for multi-class classification.\n",
    "    \"\"\"\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the softmax activation in a numerically stable manner.\n",
    "\n",
    "        :param x: Input array of shape (n_samples, n_classes).\n",
    "        :return: Softmax probabilities of the same shape.\n",
    "        \"\"\"\n",
    "        # Subtract the maximum value for numerical stability.\n",
    "        shifted_x = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_scores = np.exp(shifted_x)\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the Jacobian matrix for softmax.\n",
    "\n",
    "        This method returns a 3D tensor where each slice along the first axis\n",
    "        is the Jacobian matrix of the softmax for that sample.\n",
    "        Note: For training, softmax is usually paired with cross-entropy loss,\n",
    "        which leads to a simplified gradient computation.\n",
    "        \"\"\"\n",
    "        s = self.forward(x)\n",
    "        n, c = s.shape\n",
    "        jacobian = np.zeros((n, c, c), dtype=s.dtype)\n",
    "        # Build the full Jacobian matrix for each sample\n",
    "        for i in range(n):\n",
    "            for j in range(c):\n",
    "                for k in range(c):\n",
    "                    if j == k:\n",
    "                        jacobian[i, j, k] = s[i, j] * (1.0 - s[i, j])\n",
    "                    else:\n",
    "                        jacobian[i, j, k] = -s[i, j] * s[i, k]\n",
    "        return jacobian\n",
    "\n",
    "\n",
    "class Linear(ActivationFunction):\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Linear activation returns the input directly.\"\"\"\n",
    "        return x\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"The derivative of a linear function is 1.\"\"\"\n",
    "        return np.ones_like(x)\n",
    "\n",
    "\n",
    "class Softplus(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Softplus activation function: f(x) = ln(1 + exp(x)).\n",
    "    Acts as a smooth approximation to the ReLU function.\n",
    "    \"\"\"\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the softplus activation.\"\"\"\n",
    "        return np.log1p(np.exp(x))  # np.log1p provides numerical stability for small x\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the softplus function.\n",
    "        \n",
    "        The derivative of softplus is the sigmoid function.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class Mish(ActivationFunction):\n",
    "    \"\"\"\n",
    "    Mish activation function: f(x) = x * tanh(softplus(x)).\n",
    "    \n",
    "    Mish is a self-regularized non-monotonic activation function that has been shown\n",
    "    to improve performance in some deep learning applications.\n",
    "    \"\"\"\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the Mish activation.\"\"\"\n",
    "        return x * np.tanh(np.log1p(np.exp(x)))  # Using softplus within tanh\n",
    "\n",
    "    def derivative(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute an approximate derivative of the Mish activation.\n",
    "        \n",
    "        Note: This implementation uses a simplified derivative expression that may not be\n",
    "        mathematically exact but works in practice.\n",
    "        \"\"\"\n",
    "        softplus = np.log1p(np.exp(x))\n",
    "        tanh_sp = np.tanh(softplus)\n",
    "        # Derivative of tanh(softplus(x))\n",
    "        derivative_tanh = 1 - tanh_sp ** 2  \n",
    "        # A complex term (omega) approximates the numerator for the derivative\n",
    "        omega = 4 * (x + 1) + 4 * np.exp(2*x) + np.exp(3*x) + np.exp(x) * (4*x + 6)\n",
    "        # A term (delta) approximates the denominator for the derivative\n",
    "        delta = 2 * np.exp(2*x) + np.exp(2*x) + 2\n",
    "        return np.exp(x) * omega / delta\n",
    "\n",
    "\n",
    "##############################################################################################################################\n",
    "#                                                  Loss Functions (Abstract Base Class)                                #\n",
    "##############################################################################################################################\n",
    "\n",
    "class LossFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the loss value for a batch of predictions.\n",
    "\n",
    "        :param y_true: True target values.\n",
    "        :param y_pred: Predicted values from the network.\n",
    "        :return: A scalar loss value representing the error.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the loss function with respect to the predictions.\n",
    "\n",
    "        :param y_true: True target values.\n",
    "        :param y_pred: Predicted values.\n",
    "        :return: The gradient of the loss with respect to y_pred.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class SquaredError(LossFunction):\n",
    "    \"\"\"\n",
    "    Mean Squared Error (MSE) loss function, scaled by 1/2 for convenience.\n",
    "    \n",
    "    L = 0.5 * mean((y_pred - y_true)^2)\n",
    "    \"\"\"\n",
    "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the squared error loss.\"\"\"\n",
    "        return 0.5 * np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the squared error loss.\n",
    "        \n",
    "        The derivative is (y_pred - y_true) divided by the number of samples.\n",
    "        \"\"\"\n",
    "        n = y_true.shape[0]\n",
    "        return (y_pred - y_true) / n\n",
    "\n",
    "\n",
    "class CrossEntropy(LossFunction):\n",
    "    \"\"\"\n",
    "    Multi-class cross-entropy loss function.\n",
    "    \n",
    "    L = -1/n * sum_over_samples( sum_over_classes( y_true * log(y_pred) ) )\n",
    "    \"\"\"\n",
    "    def loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss.\n",
    "        \n",
    "        :param y_true: True labels (one-hot encoded or distribution).\n",
    "        :param y_pred: Predicted probabilities.\n",
    "        :return: The mean cross-entropy loss.\n",
    "        \"\"\"\n",
    "        eps = 1e-9  # Small epsilon to prevent log(0)\n",
    "        # Clip predictions to avoid numerical issues with log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, eps, 1.0 - eps)\n",
    "        return -np.mean(np.sum(y_true * np.log(y_pred_clipped), axis=1))\n",
    "\n",
    "    def derivative(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the derivative of the cross-entropy loss.\n",
    "        \n",
    "        For softmax outputs, the derivative simplifies to (y_pred - y_true) / n.\n",
    "        \"\"\"\n",
    "        n = y_true.shape[0]\n",
    "        return (y_pred - y_true) / n\n",
    "\n",
    "\n",
    "##############################################################################################################################\n",
    "#                                                        Layer                                                           #\n",
    "##############################################################################################################################\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, fan_in: int, fan_out: int, activation_function: ActivationFunction, dropout_rate: float = 0.0):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer with the given parameters.\n",
    "\n",
    "        This method initializes the weight matrix using Glorot (Xavier) uniform initialization,\n",
    "        sets up biases, and stores the activation function and dropout configuration.\n",
    "\n",
    "        :param fan_in: Number of input neurons from the previous layer.\n",
    "        :param fan_out: Number of neurons in the current layer.\n",
    "        :param activation_function: Instance of an ActivationFunction to apply.\n",
    "        :param dropout_rate: Fraction of neurons to drop during training (0.0 means no dropout).\n",
    "        \"\"\"\n",
    "        self.fan_in = fan_in\n",
    "        self.fan_out = fan_out\n",
    "        self.activation_function = activation_function\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Placeholders for storing activations and dropout mask during forward pass.\n",
    "        self.activations = None\n",
    "        self.dropout_mask = None\n",
    "        # Placeholder for storing the backpropagated error term.\n",
    "        self.delta = None\n",
    "\n",
    "        # Glorot (Xavier) uniform initialization for weights.\n",
    "        limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        self.W = np.random.uniform(-limit, limit, (fan_in, fan_out))\n",
    "        # Initialize biases to zero.\n",
    "        self.b = np.zeros((1, fan_out))\n",
    "\n",
    "    def forward(self, h: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform the forward pass for the layer.\n",
    "\n",
    "        This method computes the pre-activation (z = hW + b), applies the activation function,\n",
    "        and optionally applies dropout during training.\n",
    "\n",
    "        :param h: Input data to the layer (shape: [batch_size, fan_in]).\n",
    "        :param training: Flag to determine if dropout should be applied.\n",
    "        :return: Output after applying the activation function (and dropout if training).\n",
    "        \"\"\"\n",
    "        z = np.dot(h, self.W) + self.b  # Compute the linear transformation.\n",
    "        a = self.activation_function.forward(z)  # Apply activation function.\n",
    "        \n",
    "        # Apply dropout if in training mode and dropout rate > 0.\n",
    "        if training and self.dropout_rate > 0.0:\n",
    "            # Create an inverted dropout mask: 1 with probability (1 - dropout_rate),\n",
    "            # scaled to maintain activation magnitude.\n",
    "            self.dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, size=a.shape) / (1 - self.dropout_rate)\n",
    "            a = a * self.dropout_mask\n",
    "        else:\n",
    "            self.dropout_mask = None\n",
    "        \n",
    "        self.activations = a  # Store activations for use during backpropagation.\n",
    "        return a\n",
    "\n",
    "    def backward(self, h: np.ndarray, delta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Perform the backward pass for the layer.\n",
    "\n",
    "        This method computes the gradients of the loss with respect to the weights and biases,\n",
    "        using the derivative of the activation function and the chain rule.\n",
    "\n",
    "        :param h: Input to the layer during the forward pass (shape: [batch_size, fan_in]).\n",
    "        :param delta: The gradient of the loss with respect to this layer's output (shape: [batch_size, fan_out]).\n",
    "        :return: A tuple (dL_dW, dL_db) containing the gradients for weights and biases.\n",
    "        \"\"\"\n",
    "        # If dropout was applied during the forward pass, adjust the delta accordingly.\n",
    "        if self.dropout_mask is not None:\n",
    "            delta = delta * self.dropout_mask\n",
    "\n",
    "        # Recompute pre-activation (z) to calculate the derivative.\n",
    "        z = np.dot(h, self.W) + self.b\n",
    "        d_act = self.activation_function.derivative(z)\n",
    "        # Apply element-wise multiplication for chain rule.\n",
    "        d_out = delta * d_act\n",
    "\n",
    "        # Compute gradient with respect to weights: dL/dW = h^T * d_out.\n",
    "        dL_dW = np.dot(h.T, d_out)\n",
    "        # Compute gradient with respect to biases: sum over the batch dimension.\n",
    "        dL_db = np.sum(d_out, axis=0, keepdims=True)\n",
    "\n",
    "        # Compute delta to propagate to the previous layer: d_in = d_out * W^T.\n",
    "        self.delta = np.dot(d_out, self.W.T)\n",
    "        return dL_dW, dL_db\n",
    "\n",
    "\n",
    "##############################################################################################################################\n",
    "#                                        Multilayer Perceptron (MLP)                                                   #\n",
    "##############################################################################################################################\n",
    "\n",
    "class MultilayerPerceptron:\n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        \"\"\"\n",
    "        Initialize the Multilayer Perceptron (MLP).\n",
    "\n",
    "        :param layers: List of Layer objects in order from the first to the last layer.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        # This will hold intermediate activations during the forward pass, used in backpropagation.\n",
    "        self.h_list = []\n",
    "\n",
    "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform forward propagation through the entire network.\n",
    "\n",
    "        The method stores intermediate layer activations for later use in backpropagation.\n",
    "\n",
    "        :param x: Input data to the network.\n",
    "        :param training: Flag indicating whether the model is in training mode (affects dropout).\n",
    "        :return: The network's output.\n",
    "        \"\"\"\n",
    "        self.h_list = [x]  # The input itself is the first element in the activation list.\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out, training=training)\n",
    "            self.h_list.append(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, loss_grad: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        \"\"\"\n",
    "        Perform backpropagation through the network.\n",
    "\n",
    "        This method uses the stored activations from the forward pass to compute gradients for each layer.\n",
    "\n",
    "        :param loss_grad: Gradient of the loss with respect to the network's output.\n",
    "        :return: Two lists containing the gradients with respect to the weights and biases for each layer.\n",
    "        \"\"\"\n",
    "        dl_dw_all = []\n",
    "        dl_db_all = []\n",
    "        delta = loss_grad  # Start with the loss gradient at the output.\n",
    "\n",
    "        # Loop backwards over layers using stored activations.\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            layer = self.layers[i]\n",
    "            h = self.h_list[i]  # Retrieve the input to this layer.\n",
    "            dW, dB = layer.backward(h, delta)\n",
    "            dl_dw_all.insert(0, dW)  # Prepend gradients to maintain original layer order.\n",
    "            dl_db_all.insert(0, dB)\n",
    "            delta = layer.delta  # Set delta for the next iteration.\n",
    "        return dl_dw_all, dl_db_all\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        train_x: np.ndarray, \n",
    "        train_y: np.ndarray, \n",
    "        val_x: np.ndarray, \n",
    "        val_y: np.ndarray, \n",
    "        loss_func: LossFunction, \n",
    "        learning_rate: float = 1E-3, \n",
    "        batch_size: int = 16, \n",
    "        epochs: int = 32,\n",
    "        momentum: float = 0.0,\n",
    "        rmsprop: bool = False,\n",
    "        rmsprop_decay: float = 0.9,\n",
    "        epsilon: float = 1e-8\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Train the MLP using mini-batch stochastic gradient descent (SGD).\n",
    "\n",
    "        This method supports optional momentum and RMSProp optimizers.\n",
    "\n",
    "        :param train_x: Training set inputs.\n",
    "        :param train_y: Training set outputs.\n",
    "        :param val_x: Validation set inputs.\n",
    "        :param val_y: Validation set outputs.\n",
    "        :param loss_func: Instance of a LossFunction to compute the error.\n",
    "        :param learning_rate: Step size for weight updates.\n",
    "        :param batch_size: Number of samples per batch.\n",
    "        :param epochs: Total number of training epochs.\n",
    "        :param momentum: Momentum factor; if > 0.0, momentum updates will be used.\n",
    "        :param rmsprop: If True, RMSProp optimizer is used instead of vanilla SGD.\n",
    "        :param rmsprop_decay: Decay rate for the RMSProp optimizer.\n",
    "        :param epsilon: A small constant for numerical stability in RMSProp.\n",
    "        :return: Two arrays containing the training and validation loss per epoch.\n",
    "        \"\"\"\n",
    "        n_samples = train_x.shape[0]\n",
    "        training_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        # Initialize velocity (for momentum) for each layer.\n",
    "        velocities_W = [np.zeros_like(layer.W) for layer in self.layers]\n",
    "        velocities_b = [np.zeros_like(layer.b) for layer in self.layers]\n",
    "        # Initialize caches for RMSProp if enabled.\n",
    "        if rmsprop:\n",
    "            caches_W = [np.zeros_like(layer.W) for layer in self.layers]\n",
    "            caches_b = [np.zeros_like(layer.b) for layer in self.layers]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the training data at the start of each epoch.\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            train_x_shuffled = train_x[indices]\n",
    "            train_y_shuffled = train_y[indices]\n",
    "\n",
    "            total_train_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            # Process each mini-batch.\n",
    "            for batch_x, batch_y in batch_generator(train_x_shuffled, train_y_shuffled, batch_size):\n",
    "                # Forward pass: compute the network output.\n",
    "                y_pred = self.forward(batch_x, training=True)\n",
    "                # Calculate the loss for the current batch.\n",
    "                loss_val = loss_func.loss(batch_y, y_pred)\n",
    "                total_train_loss += loss_val\n",
    "                num_batches += 1\n",
    "\n",
    "                # Compute the gradient of the loss with respect to the output.\n",
    "                loss_gradient = loss_func.derivative(batch_y, y_pred)\n",
    "                # Backward pass: compute gradients for all layers.\n",
    "                dW_all, dB_all = self.backward(loss_gradient)\n",
    "\n",
    "                # Update weights and biases for each layer.\n",
    "                for i, layer in enumerate(self.layers):\n",
    "                    if rmsprop:\n",
    "                        # Update caches for RMSProp.\n",
    "                        caches_W[i] = rmsprop_decay * caches_W[i] + (1 - rmsprop_decay) * (dW_all[i] ** 2)\n",
    "                        caches_b[i] = rmsprop_decay * caches_b[i] + (1 - rmsprop_decay) * (dB_all[i] ** 2)\n",
    "                        # RMSProp update rule.\n",
    "                        layer.W -= learning_rate * dW_all[i] / (np.sqrt(caches_W[i]) + epsilon)\n",
    "                        layer.b -= learning_rate * dB_all[i] / (np.sqrt(caches_b[i]) + epsilon)\n",
    "                    elif momentum > 0.0:\n",
    "                        # Momentum update rule.\n",
    "                        velocities_W[i] = momentum * velocities_W[i] - learning_rate * dW_all[i]\n",
    "                        velocities_b[i] = momentum * velocities_b[i] - learning_rate * dB_all[i]\n",
    "                        layer.W += velocities_W[i]\n",
    "                        layer.b += velocities_b[i]\n",
    "                    else:\n",
    "                        # Standard SGD update.\n",
    "                        layer.W -= learning_rate * dW_all[i]\n",
    "                        layer.b -= learning_rate * dB_all[i]\n",
    "\n",
    "            # Compute average training loss for this epoch.\n",
    "            avg_train_loss = total_train_loss / num_batches\n",
    "            training_losses.append(avg_train_loss)\n",
    "\n",
    "            # Evaluate on validation set (disable dropout for evaluation).\n",
    "            val_pred = self.forward(val_x, training=False)\n",
    "            val_loss = loss_func.loss(val_y, val_pred)\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        return np.array(training_losses), np.array(validation_losses)\n",
    "\n",
    "\n",
    "def plot_losses(training_losses: np.ndarray, validation_losses: np.ndarray):\n",
    "    \"\"\"\n",
    "    Plot the training and validation loss curves over epochs.\n",
    "\n",
    "    :param training_losses: Array of training loss values per epoch.\n",
    "    :param validation_losses: Array of validation loss values per epoch.\n",
    "    \"\"\"\n",
    "    epochs = np.arange(1, len(training_losses) + 1)\n",
    "    plt.plot(epochs, training_losses, label=\"Training Loss\")\n",
    "    plt.plot(epochs, validation_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Optional: Test the MLP with dummy data when running this script directly.\n",
    "if __name__ == \"__main__\":\n",
    "    # Set a random seed for reproducibility.\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate random dummy data for a regression task.\n",
    "    X = np.random.rand(200, 10)  # 200 samples, 10 features each.\n",
    "    y = np.random.rand(200, 1)   # 200 target values.\n",
    "\n",
    "    # Split the data into training (80%) and validation (20%) sets.\n",
    "    split = int(0.8 * X.shape[0])\n",
    "    train_X, val_X = X[:split], X[split:]\n",
    "    train_y, val_y = y[:split], y[split:]\n",
    "\n",
    "    # Define a simple MLP with one hidden layer using ReLU activation and dropout.\n",
    "    hidden_layer = Layer(fan_in=10, fan_out=20, activation_function=Relu(), dropout_rate=0.1)\n",
    "    # Output layer with linear activation for regression.\n",
    "    output_layer = Layer(fan_in=20, fan_out=1, activation_function=Linear())\n",
    "    \n",
    "    # Instantiate the MLP with the defined layers.\n",
    "    mlp = MultilayerPerceptron(layers=[hidden_layer, output_layer])\n",
    "\n",
    "    # Select the SquaredError loss function for this regression problem.\n",
    "    loss_function = SquaredError()\n",
    "\n",
    "    # Train the model; you can experiment with momentum and RMSProp by toggling parameters.\n",
    "    training_losses, validation_losses = mlp.train(\n",
    "        train_x=train_X,\n",
    "        train_y=train_y,\n",
    "        val_x=val_X,\n",
    "        val_y=val_y,\n",
    "        loss_func=loss_function,\n",
    "        learning_rate=1e-2,\n",
    "        batch_size=16,\n",
    "        epochs=50,\n",
    "        momentum=0.9,       # Use momentum update.\n",
    "        rmsprop=False       # Set True to use RMSProp instead.\n",
    "    )\n",
    "\n",
    "    # Plot the training and validation loss curves.\n",
    "    plot_losses(training_losses, validation_losses)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
