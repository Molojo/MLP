{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This code comes from: https://www.kaggle.com/code/hojjatk/read-mnist-dataset\n",
    "#\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import numpy as np  # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path import join\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "\n",
    "    def read_images_labels(self, images_filepath, labels_filepath):\n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())\n",
    "\n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())\n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (np.array(x_train), np.array(y_train)),(np.array(x_test), np.array(y_test))\n",
    "\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = '/Users/mubaraqolojo/Downloads/MLP/archive'\n",
    "training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1\n",
    "    for x in zip(images, title_texts):\n",
    "        image = x[0]\n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)\n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize=15)\n",
    "        index += 1\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "\n",
    "np.save('/Users/mubaraqolojo/Downloads/MLP/archive/mnist-train-x.npy', x_train.reshape(len(x_train), 784))\n",
    "np.save('/Users/mubaraqolojo/Downloads/MLP/archive/mnist-train-y.npy', y_train)\n",
    "np.save('/Users/mubaraqolojo/Downloads/MLP/archive/mnist-test-x.npy', x_test.reshape(len(x_test), 784))\n",
    "np.save('/Users/mubaraqolojo/Downloads/MLP/archive/mnist-test-y.npy', y_test)\n",
    "\n",
    "#\n",
    "# Show some random training and test images\n",
    "#\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])\n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))\n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] | Train Loss: 1.944908 | Val Loss: 1.582879\n",
      "Epoch [2/20] | Train Loss: 1.383735 | Val Loss: 1.127302\n",
      "Epoch [3/20] | Train Loss: 1.050952 | Val Loss: 0.868615\n",
      "Epoch [4/20] | Train Loss: 0.860301 | Val Loss: 0.717082\n",
      "Epoch [5/20] | Train Loss: 0.743411 | Val Loss: 0.620729\n",
      "Epoch [6/20] | Train Loss: 0.665682 | Val Loss: 0.554935\n",
      "Epoch [7/20] | Train Loss: 0.610425 | Val Loss: 0.507278\n",
      "Epoch [8/20] | Train Loss: 0.569186 | Val Loss: 0.471442\n",
      "Epoch [9/20] | Train Loss: 0.537236 | Val Loss: 0.443789\n",
      "Epoch [10/20] | Train Loss: 0.511618 | Val Loss: 0.421370\n",
      "Epoch [11/20] | Train Loss: 0.490595 | Val Loss: 0.403347\n",
      "Epoch [12/20] | Train Loss: 0.472981 | Val Loss: 0.387858\n",
      "Epoch [13/20] | Train Loss: 0.457972 | Val Loss: 0.374796\n",
      "Epoch [14/20] | Train Loss: 0.444963 | Val Loss: 0.363812\n",
      "Epoch [15/20] | Train Loss: 0.433569 | Val Loss: 0.354065\n",
      "Epoch [16/20] | Train Loss: 0.423411 | Val Loss: 0.345543\n",
      "Epoch [17/20] | Train Loss: 0.414329 | Val Loss: 0.337907\n",
      "Epoch [18/20] | Train Loss: 0.406122 | Val Loss: 0.331038\n",
      "Epoch [19/20] | Train Loss: 0.398751 | Val Loss: 0.324809\n",
      "Epoch [20/20] | Train Loss: 0.391900 | Val Loss: 0.319227\n",
      "Test Accuracy: 90.16%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from array import array\n",
    "from os.path import join\n",
    "import random\n",
    "\n",
    "# Import MLP classes from your mlp.py implementation.\n",
    "# Ensure that your mlp.py includes the CrossEntropy loss and Softmax activation.\n",
    "from mlp import Layer, MultilayerPerceptron, CrossEntropy, Relu, Softmax\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "\n",
    "    def read_images_labels(self, images_filepath, labels_filepath):\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())\n",
    "\n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())\n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images.append(img)\n",
    "        return images, labels\n",
    "\n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (np.array(x_train), np.array(y_train)), (np.array(x_test), np.array(y_test))\n",
    "\n",
    "#\n",
    "# Helper function to display a list of images with titles.\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images) / cols) + 1\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    index = 1\n",
    "    for image, title_text in zip(images, title_texts):\n",
    "        plt.subplot(rows, cols, index)\n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        plt.title(title_text, fontsize=10)\n",
    "        plt.axis('off')\n",
    "        index += 1\n",
    "    plt.show()\n",
    "\n",
    "#\n",
    "# Helper function to one-hot encode label vectors.\n",
    "#\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    one_hot = np.zeros((len(labels), num_classes))\n",
    "    one_hot[np.arange(len(labels)), labels] = 1\n",
    "    return one_hot\n",
    "\n",
    "def main():\n",
    "    # Set file paths for MNIST dataset (adjust the input_path as needed)\n",
    "    input_path = '/Users/mubaraqolojo/Downloads/MLP/archive'\n",
    "    training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "    training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "    test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "    test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath,\n",
    "                                       test_images_filepath, test_labels_filepath)\n",
    "    (x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "    \n",
    "    # Preprocess images: flatten and normalize pixel values to [0, 1]\n",
    "    x_train = x_train.reshape(len(x_train), 784).astype(np.float32) / 255.0\n",
    "    x_test = x_test.reshape(len(x_test), 784).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Ensure labels are integer type.\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "    \n",
    "    # Convert labels to one-hot encoded vectors for training.\n",
    "    y_train_onehot = one_hot_encode(y_train, num_classes=10)\n",
    "    y_test_onehot = one_hot_encode(y_test, num_classes=10)\n",
    "    \n",
    "    # Split training data into training and validation sets (e.g., 90% train, 10% validation)\n",
    "    num_train = int(0.9 * len(x_train))\n",
    "    train_x, val_x = x_train[:num_train], x_train[num_train:]\n",
    "    train_y, val_y = y_train_onehot[:num_train], y_train_onehot[num_train:]\n",
    "    \n",
    "    # Build an MLP for MNIST classification: 784 -> 128 -> 10\n",
    "    layers = [\n",
    "        Layer(fan_in=784, fan_out=128, activation_function=Relu()),\n",
    "        Layer(fan_in=128, fan_out=10, activation_function=Softmax())\n",
    "    ]\n",
    "    mlp = MultilayerPerceptron(layers)\n",
    "    \n",
    "    # Use CrossEntropy loss for classification.\n",
    "    loss_func = CrossEntropy()\n",
    "    \n",
    "    # Training parameters\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 64\n",
    "    epochs = 20\n",
    "    \n",
    "    # Train the network\n",
    "    train_losses, val_losses = mlp.train(\n",
    "        train_x, train_y,\n",
    "        val_x, val_y,\n",
    "        loss_func=loss_func,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set (disable dropout during evaluation)\n",
    "    test_pred = mlp.forward(x_test, training=False)\n",
    "    # Get predicted classes as the index with highest probability\n",
    "    pred_classes = np.argmax(test_pred, axis=1)\n",
    "    # Compute test accuracy\n",
    "    accuracy = np.mean(pred_classes == y_test)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Plot training and validation loss curves\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('MNIST: Training vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display some random test images along with true and predicted labels\n",
    "    sample_indices = np.random.choice(len(x_test), 10, replace=False)\n",
    "    sample_images = x_test[sample_indices].reshape(-1, 28, 28)\n",
    "    sample_titles = [f\"True: {y_test[i]}, Pred: {pred_classes[i]}\" for i in sample_indices]\n",
    "    show_images(sample_images, sample_titles)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] | Train Loss: 0.726982 | Val Loss: 0.337220\n",
      "Epoch [2/50] | Train Loss: 0.305736 | Val Loss: 0.268559\n",
      "Epoch [3/50] | Train Loss: 0.251285 | Val Loss: 0.234920\n",
      "Epoch [4/50] | Train Loss: 0.217419 | Val Loss: 0.207540\n",
      "Epoch [5/50] | Train Loss: 0.190720 | Val Loss: 0.192421\n",
      "Epoch [6/50] | Train Loss: 0.171317 | Val Loss: 0.170475\n",
      "Epoch [7/50] | Train Loss: 0.154308 | Val Loss: 0.157571\n",
      "Epoch [8/50] | Train Loss: 0.141426 | Val Loss: 0.150460\n",
      "Epoch [9/50] | Train Loss: 0.130039 | Val Loss: 0.138400\n",
      "Epoch [10/50] | Train Loss: 0.120291 | Val Loss: 0.130073\n",
      "Epoch [11/50] | Train Loss: 0.111655 | Val Loss: 0.125563\n",
      "Epoch [12/50] | Train Loss: 0.104792 | Val Loss: 0.119592\n",
      "Epoch [13/50] | Train Loss: 0.097616 | Val Loss: 0.113599\n",
      "Epoch [14/50] | Train Loss: 0.091190 | Val Loss: 0.110832\n",
      "Epoch [15/50] | Train Loss: 0.085989 | Val Loss: 0.107209\n",
      "Epoch [16/50] | Train Loss: 0.080689 | Val Loss: 0.103732\n",
      "Epoch [17/50] | Train Loss: 0.076065 | Val Loss: 0.102098\n",
      "Epoch [18/50] | Train Loss: 0.072132 | Val Loss: 0.098419\n",
      "Epoch [19/50] | Train Loss: 0.068502 | Val Loss: 0.095316\n",
      "Epoch [20/50] | Train Loss: 0.064656 | Val Loss: 0.094365\n",
      "Epoch [21/50] | Train Loss: 0.061245 | Val Loss: 0.092820\n",
      "Epoch [22/50] | Train Loss: 0.058332 | Val Loss: 0.089821\n",
      "Epoch [23/50] | Train Loss: 0.055283 | Val Loss: 0.090216\n",
      "Epoch [24/50] | Train Loss: 0.052689 | Val Loss: 0.087941\n",
      "Epoch [25/50] | Train Loss: 0.050041 | Val Loss: 0.087619\n",
      "Epoch [26/50] | Train Loss: 0.047545 | Val Loss: 0.086948\n",
      "Epoch [27/50] | Train Loss: 0.045215 | Val Loss: 0.090620\n",
      "Epoch [28/50] | Train Loss: 0.043274 | Val Loss: 0.082930\n",
      "Epoch [29/50] | Train Loss: 0.041169 | Val Loss: 0.087143\n",
      "Epoch [30/50] | Train Loss: 0.039540 | Val Loss: 0.083809\n",
      "Epoch [31/50] | Train Loss: 0.037253 | Val Loss: 0.083037\n",
      "Epoch [32/50] | Train Loss: 0.035852 | Val Loss: 0.083727\n",
      "Epoch [33/50] | Train Loss: 0.034180 | Val Loss: 0.083112\n",
      "Epoch [34/50] | Train Loss: 0.032553 | Val Loss: 0.082658\n",
      "Epoch [35/50] | Train Loss: 0.031330 | Val Loss: 0.083713\n",
      "Epoch [36/50] | Train Loss: 0.029549 | Val Loss: 0.080694\n",
      "Epoch [37/50] | Train Loss: 0.028327 | Val Loss: 0.082238\n",
      "Epoch [38/50] | Train Loss: 0.027165 | Val Loss: 0.082065\n",
      "Epoch [39/50] | Train Loss: 0.025779 | Val Loss: 0.082526\n",
      "Epoch [40/50] | Train Loss: 0.024750 | Val Loss: 0.082241\n",
      "Epoch [41/50] | Train Loss: 0.023683 | Val Loss: 0.080076\n",
      "Epoch [42/50] | Train Loss: 0.022346 | Val Loss: 0.081712\n",
      "Epoch [43/50] | Train Loss: 0.021436 | Val Loss: 0.081873\n",
      "Epoch [44/50] | Train Loss: 0.020522 | Val Loss: 0.082069\n",
      "Epoch [45/50] | Train Loss: 0.019583 | Val Loss: 0.082197\n",
      "Epoch [46/50] | Train Loss: 0.018683 | Val Loss: 0.080275\n",
      "Epoch [47/50] | Train Loss: 0.018059 | Val Loss: 0.081594\n",
      "Epoch [48/50] | Train Loss: 0.017089 | Val Loss: 0.083899\n",
      "Epoch [49/50] | Train Loss: 0.016556 | Val Loss: 0.082529\n",
      "Epoch [50/50] | Train Loss: 0.015893 | Val Loss: 0.081626\n",
      "\n",
      "Test Accuracy: 97.74%\n",
      "\n",
      "Sample predictions:\n",
      "Image 387: Predicted = 2, True = 2\n",
      "Image 4751: Predicted = 4, True = 4\n",
      "Image 8539: Predicted = 5, True = 5\n",
      "Image 3366: Predicted = 4, True = 4\n",
      "Image 768: Predicted = 1, True = 1\n",
      "Image 1437: Predicted = 4, True = 4\n",
      "Image 7683: Predicted = 2, True = 2\n",
      "Image 3693: Predicted = 9, True = 9\n",
      "Image 4830: Predicted = 5, True = 5\n",
      "Image 7008: Predicted = 9, True = 9\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import struct\n",
    "from array import array\n",
    "from os.path import join\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import your MLP implementation (make sure mlp.py is in your working directory)\n",
    "from mlp import Layer, MultilayerPerceptron, CrossEntropy, Relu, Softmax\n",
    "\n",
    "###############################################################################\n",
    "# MNIST Data Loader Class\n",
    "###############################################################################\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath, training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "\n",
    "    def read_images_labels(self, images_filepath, labels_filepath):\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())\n",
    "\n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())\n",
    "\n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images.append(img)\n",
    "        return images, labels\n",
    "\n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (np.array(x_train), np.array(y_train)), (np.array(x_test), np.array(y_test))\n",
    "\n",
    "###############################################################################\n",
    "# Helper function to convert labels to one-hot encoding\n",
    "###############################################################################\n",
    "def one_hot(labels, num_classes=10):\n",
    "    one_hot_labels = np.zeros((len(labels), num_classes), dtype=np.float32)\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot_labels[i, int(label)] = 1.0\n",
    "    return one_hot_labels\n",
    "\n",
    "###############################################################################\n",
    "# Main training function for MNIST classification using MLP\n",
    "###############################################################################\n",
    "def main():\n",
    "    # Set file paths (adjust these paths as needed)\n",
    "    input_path = '/Users/mubaraqolojo/Downloads/MLP/archive'\n",
    "    training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "    training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "    test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "    test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath,\n",
    "                                       test_images_filepath, test_labels_filepath)\n",
    "    (x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "    # Preprocess images: flatten (28x28 -> 784) and normalize pixel values to [0,1]\n",
    "    x_train = x_train.reshape(x_train.shape[0], 784).astype(np.float32) / 255.0\n",
    "    x_test  = x_test.reshape(x_test.shape[0], 784).astype(np.float32) / 255.0\n",
    "\n",
    "    # Convert labels to one-hot encoding for training/validation\n",
    "    y_train_onehot = one_hot(y_train, num_classes=10)\n",
    "\n",
    "    # Split training data into training (90%) and validation (10%)\n",
    "    n_train = x_train.shape[0]\n",
    "    split_index = int(0.9 * n_train)\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(n_train)\n",
    "    x_train = x_train[indices]\n",
    "    y_train_onehot = y_train_onehot[indices]\n",
    "    y_train_int = y_train[indices]  # keep for potential analysis\n",
    "\n",
    "    train_x = x_train[:split_index]\n",
    "    train_y = y_train_onehot[:split_index]\n",
    "    val_x = x_train[split_index:]\n",
    "    val_y = y_train_onehot[split_index:]\n",
    "\n",
    "    # Design the MLP architecture for MNIST classification:\n",
    "    # Input layer: 784 neurons; two hidden layers; output layer: 10 neurons with Softmax activation.\n",
    "    layers = [\n",
    "        Layer(fan_in=784, fan_out=128, activation_function=Relu()),\n",
    "        Layer(fan_in=128, fan_out=64, activation_function=Relu()),\n",
    "        Layer(fan_in=64, fan_out=10, activation_function=Softmax())\n",
    "    ]\n",
    "    mlp = MultilayerPerceptron(layers)\n",
    "\n",
    "    # Loss function: use CrossEntropy for classification.\n",
    "    loss_func = CrossEntropy()\n",
    "\n",
    "    # Training hyperparameters (tune these for best performance)\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "\n",
    "    # Train the model (training and validation loss will be printed each epoch)\n",
    "    train_losses, val_losses = mlp.train(\n",
    "        train_x, train_y,\n",
    "        val_x, val_y,\n",
    "        loss_func=loss_func,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        momentum=0.9  # try adding momentum if desired\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the full test set (disable dropout)\n",
    "    test_pred = mlp.forward(x_test, training=False)\n",
    "    # For each test example, the predicted class is the index with maximum probability.\n",
    "    predicted_labels = np.argmax(test_pred, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == y_test)\n",
    "    print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "    # Plot training & validation loss curves\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('MNIST: Training vs. Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Optionally, display some sample predictions\n",
    "    sample_indices = np.random.choice(x_test.shape[0], 10, replace=False)\n",
    "    print(\"\\nSample predictions:\")\n",
    "    for idx in sample_indices:\n",
    "        print(f\"Image {idx}: Predicted = {predicted_labels[idx]}, True = {y_test[idx]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
